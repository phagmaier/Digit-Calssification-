{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1df4ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "train = pd.read_csv('mnist_train.csv')\n",
    "test = pd.read_csv('mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2630c5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[2 rows x 785 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f18858",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train['label'].values\n",
    "test_labels = test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78006cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('label', axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef8ecc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop('label', axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f461d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.values\n",
    "y = test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9f432f",
   "metadata": {},
   "source": [
    "# STOCHASTIC VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc84001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "class NN:\n",
    "    def __init__(self,X=X,y=train_labels,testX=y,testy=test_labels,output_size=10,\n",
    "                 iterations=10,lr=0.1,size=2500):\n",
    "        \n",
    "        #When testing ideal params you're going to want to reduce\n",
    "        #the size of X from 60k to like 1k just to see what works\n",
    "        #best can even go smaller maybe 500 for the first couple\n",
    "        np.random.seed(69)\n",
    "        #limiting size to 100 for testing\n",
    "        #change to larger if you want 100 should be plenty\n",
    "        self.lr = lr\n",
    "        self.X = X[:size] \n",
    "        self.y = y[:size]\n",
    "        self.testX=testX[:size]\n",
    "        self.testy=testy[:size]\n",
    "        self.w1 = np.random.rand(len(self.X[0]),len(self.X[0])) * np.sqrt(1 / len(self.X[0]))\n",
    "        self.w2 = np.random.rand(len(self.X[0]), output_size)* np.sqrt(1 / output_size)\n",
    "        self.bias = np.random.rand()\n",
    "        self.output_size = output_size\n",
    "        self.bias2 = np.random.rand()\n",
    "        #gonan try to do activation function to both \n",
    "        #hidden and final we can adjust this\n",
    "        #self.activation = np.vectorize(lambda x: 1 / (1 + np.exp(-x)))\n",
    "        #self.deriv = np.vectorize(lambda x: self.activation(x) * (1 - self.activation(x)))\n",
    "        self.activation = expit\n",
    "        self.deriv = lambda x: self.activation(x) * (1 - self.activation(x))\n",
    "        self.count = 1\n",
    "        for i in range(iterations):\n",
    "            self.right_or_wrong = []\n",
    "            for x, y in zip(self.X,self.y):\n",
    "                hidden,final = self.forward(x.reshape(1,-1))\n",
    "                cost = self.back(hidden,final,y,x.reshape(1,-1))\n",
    "                answer = final.argmax()\n",
    "                #print(f'answer: {answer} correct value: {y}')\n",
    "                self.right_or_wrong.append(1) if answer == y else self.right_or_wrong.append(0)\n",
    "            self.count+=1\n",
    "            if self.count % 10 == 0:\n",
    "                print(f'The cost after iteration {self.count}: {cost}')\n",
    "                print(f'Correctly classified {sum(self.right_or_wrong)/len(self.right_or_wrong) * 100}%')\n",
    "                print('------------------------------------------------------')\n",
    "    \n",
    "    def forward(self,X):\n",
    "        #in the example you only apply the activation func\n",
    "        #in the final output i'll try both\n",
    "        hidden_output = self.activation(X @ self.w1 + self.bias)\n",
    "        final_output = self.activation(hidden_output @ self.w2 + self.bias2)\n",
    "        return hidden_output, final_output\n",
    "        \n",
    "    def back(self,hidden_output,final_outputs,y,x):\n",
    "        #y = np.array([0 if i == 0 else 1 for i in range(self.output_size)])\n",
    "        y = np.array([0 if i!= y else 1 for i in range(self.output_size)])\n",
    "        results = final_outputs - y\n",
    "        cost = np.sum((final_outputs - y)**2) / len(results)\n",
    "        #don't think i need this\n",
    "        #you only need the difference between outputs\n",
    "        #so in ours it's just results\n",
    "        #delta_output = results * self.deriv(results)\n",
    "        \n",
    "        #THIS MAY BE ALL KIND OF FUCKED UP\n",
    "        \n",
    "        #layer_2_delta is just results\n",
    "        #layer_1_delta = results @ self.w1.T * self.deriv(hidden_output)\n",
    "        layer_1_delta = results @ self.w2.T * self.deriv(hidden_output)\n",
    "        \n",
    "        #delta_hidden = results @ self.w2.T * self.deriv(hidden_output)\n",
    "        self.w2 -= self.lr * hidden_output.T @ results\n",
    "        self.w1 -= self.lr * x.T @ layer_1_delta\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def testing(self):\n",
    "        #self.testX=testX[:100]\n",
    "        #self.testy=testy[:100]\n",
    "        results = []\n",
    "        for i,y in zip(self.testX,self.testy):\n",
    "            one = self.activation(i.reshape(1,-1) @ self.w1 + self.bias)\n",
    "            two = self.activation(one @ self.w2 + self.bias2)\n",
    "            answer = two.argmax()\n",
    "            results.append(1) if answer == y else results.append(0)\n",
    "        print('TESTING RESULTS')\n",
    "        print('----------------------------------')\n",
    "        print(f'Percentage of correct classification: {sum(results)/len(results) * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57662a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testNN = NN(iterations=1000, size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e343682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testNN.testing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978b3645",
   "metadata": {},
   "source": [
    "# BATCHED VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "557b793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "from scipy.special import expit\n",
    "class BatchNN:\n",
    "    def __init__(self,X=X,y=train_labels,testX=y,testy=test_labels,\n",
    "                 output_size=10,iterations=100,lr=.1, batch_size=10, size=2500):\n",
    "        \n",
    "        np.random.seed(69)\n",
    "        self.lr = lr\n",
    "        self.X,self.y = self.batchify(X[:size], y[:size],batch_size)\n",
    "        #self.testX,self.testy = self.batchify(testX[:size], testy[:size],batch_size)\n",
    "        self.testX = testX[:size]\n",
    "        self.testy = testy[:size]\n",
    "        self.w1 = np.random.rand(len(self.X[0][0]),len(self.X[0][0])) * np.sqrt(1 / len(self.X[0][0]))\n",
    "        self.w2 = np.random.rand(len(self.X[0][0]), output_size)* np.sqrt(1 / output_size)\n",
    "        self.bias = np.random.rand()\n",
    "        self.bias2 = np.random.rand()\n",
    "        self.output_size = output_size\n",
    "        #gonan try to do activation function to both \n",
    "        #hidden and final we can adjust this\n",
    "        #self.activation = np.vectorize(lambda x: 1 / (1 + np.exp(-x)))\n",
    "        #self.deriv = np.vectorize(lambda x: self.activation(x) * (1 - self.activation(x)))\n",
    "        self.activation = expit\n",
    "        self.deriv = lambda x: self.activation(x) * (1 - self.activation(x))\n",
    "        self.count = 1\n",
    "        \n",
    "        \n",
    "        for i in range(iterations):\n",
    "            self.right_or_wrong = []\n",
    "            for batch,Y in zip(self.X,self.y):\n",
    "                batch_size = len(batch)\n",
    "                batch_x = 0\n",
    "                batch_hidden = 0\n",
    "                errors = 0\n",
    "                cost = 0\n",
    "                for x,y in zip(batch,Y):\n",
    "                    batch_x += x\n",
    "                    hidden,final = self.forward(x.reshape(1,-1))\n",
    "                    batch_hidden += hidden\n",
    "                    temp_cost,temp_errors = self.back(hidden,final,y,x.reshape(1,-1))\n",
    "                    errors += temp_errors\n",
    "                    cost += temp_cost\n",
    "                    self.right_or_wrong.append(1) if final.argmax() == y else self.right_or_wrong.append(0)\n",
    "                self.update_weights(errors/batch_size,batch_hidden/batch_size,(batch_x/batch_size).reshape(1,-1))\n",
    "            cost/= len(batch) \n",
    "            self.count += 1\n",
    "            if self.count % 10 == 0:\n",
    "                print(f'The cost after iteration {self.count}: {cost}')\n",
    "                print(f'Correctly classified {sum(self.right_or_wrong)/len(self.right_or_wrong) * 100}%')\n",
    "                print('------------------------------------------------------')\n",
    "                    \n",
    "    def update_weights(self,errors,hidden_output,x):\n",
    "        layer_1_delta = (errors @ self.w2.T) * self.deriv(hidden_output)\n",
    "        self.w2 -=  (hidden_output.T @ errors) * self.lr\n",
    "        self.w1 -= (x.T @ layer_1_delta) * self.lr\n",
    "    \n",
    "    def batchify(self,X, y, batch_size):\n",
    "        assert len(X) == len(y), \"Input arrays must have the same length.\"\n",
    "        assert batch_size > 0, \"Batch size must be a positive integer.\"\n",
    "\n",
    "        num_samples = len(X)\n",
    "        num_batches = int(np.ceil(num_samples / batch_size))\n",
    "\n",
    "        batched_X = [np.array(x) for x in np.array_split(X, num_batches)]\n",
    "        batched_y = [np.array(y) for y in np.array_split(y, num_batches)]\n",
    "\n",
    "        return np.array(batched_X), np.array(batched_y)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        hidden_output = self.activation(X @ self.w1 + self.bias)\n",
    "        final_output = self.activation(hidden_output @ self.w2 + self.bias2)\n",
    "        return hidden_output, final_output\n",
    "        \n",
    "    def back(self,hidden_output,final_outputs,y,x):\n",
    "        \n",
    "        y = np.array([0 if i!= y else 1 for i in range(self.output_size)])\n",
    "        results = final_outputs - y\n",
    "        cost = np.sum((final_outputs - y)**2) / len(results)\n",
    "        \n",
    "        layer_1_delta = results @ self.w2.T * self.deriv(hidden_output)\n",
    "        \n",
    "        \n",
    "        #self.w2 -= self.lr * hidden_output.T @ results\n",
    "        #self.w1 -= self.lr * x.T @ layer_1_delta\n",
    "        return cost,results\n",
    "    \n",
    "    \n",
    "    def testing(self):\n",
    "        #self.testX=testX[:100]\n",
    "        #self.testy=testy[:100]\n",
    "        results = []\n",
    "        for i,y in zip(self.testX,self.testy):\n",
    "            one = self.activation(i.reshape(1,-1) @ self.w1 + self.bias)\n",
    "            two = self.activation(one @ self.w2 + self.bias2)\n",
    "            answer = two.argmax()\n",
    "            results.append(1) if answer == y else results.append(0)\n",
    "        print('TESTING RESULTS')\n",
    "        print('----------------------------------')\n",
    "        print(f'Percentage of correct classification: {sum(results)/len(results) * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a257c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batchedNN = BatchNN(iterations=1000,batch_size=25, lr=.1, size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e34b92e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batchedNN.testing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaca47c",
   "metadata": {},
   "source": [
    "# ADDING A FINITE DIFFERENCE TO CALCULATE BIAS UPDATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df3ab3a",
   "metadata": {},
   "source": [
    "1. CHANGE = HOW YOU ARE ALREADY HANDELING RESULTS IN UPDATES\n",
    "2. ADD AN EPS = 0.01\n",
    "3. PASS BACK EACH BIAS TO THE FORWARD PASS (USING THE AVERAGES OF THE INPUTS OF X ADD EPS TO BIAS\n",
    "4. DO THIS FOR BOTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e96527e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "class BatchNN:\n",
    "    def __init__(self,X=X,y=train_labels,testX=y,testy=test_labels,\n",
    "                 output_size=10,iterations=100,lr=0.1, batch_size=10, size=2500, eps=.1):\n",
    "        \n",
    "        np.random.seed(69)\n",
    "        self.eps = eps\n",
    "        self.lr = lr\n",
    "        self.X,self.y = self.batchify(X[:size], y[:size],batch_size)\n",
    "        #self.testX,self.testy = self.batchify(testX[:size], testy[:size],batch_size)\n",
    "        self.testX = testX[:size]\n",
    "        self.testy = testy[:size]\n",
    "        self.w1 = np.random.rand(len(self.X[0][0]),len(self.X[0][0])) * np.sqrt(1 / len(self.X[0][0]))\n",
    "        self.w2 = np.random.rand(len(self.X[0][0]), output_size)* np.sqrt(1 / output_size)\n",
    "        self.bias = np.random.rand()\n",
    "        self.bias2 = np.random.rand()\n",
    "        self.output_size = output_size\n",
    "        self.activation = expit\n",
    "        self.deriv = lambda x: self.activation(x) * (1 - self.activation(x))\n",
    "        self.count = 1\n",
    "        \n",
    "        \n",
    "        for i in range(iterations):\n",
    "            self.right_or_wrong = []\n",
    "            for batch,Y in zip(self.X,self.y):\n",
    "                batch_size = len(batch)\n",
    "                batch_x = 0\n",
    "                batch_hidden = 0\n",
    "                errors = 0\n",
    "                cost = 0\n",
    "                for x,y in zip(batch,Y):\n",
    "                    batch_x += x\n",
    "                    hidden,final = self.forward(x.reshape(1,-1))\n",
    "                    batch_hidden += hidden\n",
    "                    temp_cost,temp_errors = self.back(hidden,final,y,x.reshape(1,-1))\n",
    "                    errors += temp_errors\n",
    "                    cost += temp_cost\n",
    "                    self.right_or_wrong.append(1) if final.argmax() == y else self.right_or_wrong.append(0)\n",
    "                self.update_weights(errors/batch_size,batch_hidden/batch_size,(batch_x/batch_size).reshape(1,-1))\n",
    "                #NEW\n",
    "                self.update_bias(errors/batch_size,batch_hidden/batch_size,(batch_x/batch_size).reshape(1,-1))\n",
    "            cost/= len(batch) \n",
    "            self.count += 1\n",
    "            if self.count % 10 == 0:\n",
    "                print(f'The cost after iteration {self.count}: {cost}')\n",
    "                print(f'Correctly classified {sum(self.right_or_wrong)/len(self.right_or_wrong) * 100}%')\n",
    "                print('------------------------------------------------------')\n",
    "\n",
    "   \n",
    "    \n",
    "    def update_bias(self, cost, hidden_output, x):\n",
    "        true_f = self.activation(hidden_output @ self.w2 + self.bias2)\n",
    "\n",
    "        bias_grad = np.mean(true_f) - np.mean(self.activation(hidden_output @ self.w2 + (self.bias2 + self.eps)))\n",
    "\n",
    "        self.bias -= (bias_grad / self.eps) * self.lr\n",
    "        self.bias2 -= (bias_grad / self.eps) * self.lr\n",
    "\n",
    "        \n",
    "                    \n",
    "    def update_weights(self,errors,hidden_output,x):\n",
    "        layer_1_delta = (errors @ self.w2.T) * self.deriv(hidden_output)\n",
    "        self.w2 -=  (hidden_output.T @ errors) * self.lr\n",
    "        self.w1 -= (x.T @ layer_1_delta) * self.lr\n",
    "    \n",
    "    def batchify(self,X, y, batch_size):\n",
    "        assert len(X) == len(y), \"Input arrays must have the same length.\"\n",
    "        assert batch_size > 0, \"Batch size must be a positive integer.\"\n",
    "\n",
    "        num_samples = len(X)\n",
    "        num_batches = int(np.ceil(num_samples / batch_size))\n",
    "\n",
    "        batched_X = [np.array(x) for x in np.array_split(X, num_batches)]\n",
    "        batched_y = [np.array(y) for y in np.array_split(y, num_batches)]\n",
    "\n",
    "        return np.array(batched_X), np.array(batched_y)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        hidden_output = self.activation(X @ self.w1 + self.bias)\n",
    "        final_output = self.activation(hidden_output @ self.w2 + self.bias2)\n",
    "        return hidden_output, final_output\n",
    "        \n",
    "    def back(self,hidden_output,final_outputs,y,x):\n",
    "        \n",
    "        y = np.array([0 if i!= y else 1 for i in range(self.output_size)])\n",
    "        results = final_outputs - y\n",
    "        cost = np.sum((final_outputs - y)**2) / len(results)\n",
    "        \n",
    "        layer_1_delta = results @ self.w2.T * self.deriv(hidden_output)\n",
    "        \n",
    "        \n",
    "        #self.w2 -= self.lr * hidden_output.T @ results\n",
    "        #self.w1 -= self.lr * x.T @ layer_1_delta\n",
    "        return cost,results\n",
    "    \n",
    "    \n",
    "    def testing(self):\n",
    "        #self.testX=testX[:100]\n",
    "        #self.testy=testy[:100]\n",
    "        results = []\n",
    "        for i,y in zip(self.testX,self.testy):\n",
    "            one = self.activation(i.reshape(1,-1) @ self.w1 + self.bias)\n",
    "            two = self.activation(one @ self.w2 + self.bias2)\n",
    "            answer = two.argmax()\n",
    "            results.append(1) if answer == y else results.append(0)\n",
    "        print('TESTING RESULTS')\n",
    "        print('----------------------------------')\n",
    "        print(f'Percentage of correct classification: {sum(results)/len(results) * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02af8b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batchedNN = BatchNN(iterations=1000,batch_size=25, lr=.1, size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17199b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batchedNN.testing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c65dec8",
   "metadata": {},
   "source": [
    "# Stochastic version With Bias update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cafcb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batchedNN = BatchNN(iterations=1000,batch_size=5, lr=.1, size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5debe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batchedNN.testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "257ced23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final is the output of last layer\n",
    "#net_output_1 = layer_1\n",
    "def back_prop(w1,w2,inputs,final,targets,net_output_1,lr=0.5):\n",
    "    #IN REAL REVSION YOU'll just have adjust by be a 0\n",
    "    # and then you'll just accumulate it and then divide by whatever\n",
    "    # The batch size is\n",
    "    updated_w1 = np.copy(w1)\n",
    "    updated_w2 = np.copy(w2)\n",
    "    deriv_error = np.vectorize(lambda x,y: -1*(y-x))\n",
    "    deriv_sig_func = np.vectorize(lambda x: x * (1-x))\n",
    "    \n",
    "    #W2 update\n",
    "    \n",
    "    #GRADIENT OF ERROR\n",
    "    #IN REAL VERSION GOING TO HAVE TO STACK IT\n",
    "    #Don't know how many times I guess till it's the same shape \n",
    "    #AS W2\n",
    "    deriv_error = np.vectorize(lambda x,y: -1*(y-x))\n",
    "    error_derivs = deriv_error(final,targets)\n",
    "    error_derivs = np.vstack(np.repeat(error_derivs,2,axis=0))\n",
    "    \n",
    "    #DERIV OF OUTPUT \n",
    "    sig_deriv = deriv_sig_func(final)\n",
    "    sig_deriv = np.vstack(np.repeat(sig_deriv,2,axis=0))\n",
    "    \n",
    "    #DERIV OD PREVIOUS LAYER's raw output\n",
    "    output_layer_1 = np.vstack(np.repeat(net_output_1, 2, axis=0))\n",
    "    \n",
    "    #said above gonna have two adjusted by vars for 1 and 2\n",
    "    #if bathed we will acummuluate\n",
    "    adjust_by = raw_output_1 * sig_deriv * error_derivs\n",
    "    \n",
    "    #Won't this won't be in batched version\n",
    "    updated_w2 = (w2.T - adjust_by * lr).T\n",
    "    \n",
    "    # \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "379c556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([.05,.1]).reshape(1,-1)\n",
    "w1 = np.array([.15,.2,.25,.3]).reshape(2,2)\n",
    "w2 = np.array([.4,.45,.5,.55]).reshape(2,2)\n",
    "outputs = np.array([.01,.99]).reshape(1,-1)\n",
    "b1 = .35\n",
    "b2 = .6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d14adf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inputs=inputs,w1=w1,w2=w2,labels=outputs,b1=b1,b2=b2):\n",
    "    sig = np.vectorize(lambda x: 1/ (1+ np.exp(-x)))\n",
    "    \n",
    "    #LAYER 1\n",
    "    layer_1 = inputs @ w1.T + b1\n",
    "    layer_1_output = sig(layer_1)\n",
    "    print(f'raw output layer 1:{layer_1}')\n",
    "    print(f'activated output layer 1: {layer_1_output}')\n",
    "    \n",
    "    #LAYER 2\n",
    "    layer_2 = layer_1_output @ w2.T + b2\n",
    "    layer_2_output = sig(layer_2)\n",
    "    print(f'raw output layer 2:{layer_2}')\n",
    "    print(f'activated output layer 2: {layer_2_output}')\n",
    "    \n",
    "    #ERROR\n",
    "    erros_func = np.vectorize(lambda x,y: .5 *(y - x)**2)\n",
    "    errors = erros_func(layer_2_output, labels)\n",
    "    total_error = errors.sum()\n",
    "    print(f'individual errors: {errors}')\n",
    "    print(f'Total error: {total_error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf3618b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
